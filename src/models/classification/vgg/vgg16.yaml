# config.yaml

# VGG16 Configuration

# parameters
nc: 1000  # number of classes

# backbone
backbone:
  [
   # Block 1
   [-1, 1, Conv, [64, 3, 1, 1]],  # 0-Conv1_1
   [-1, 1, Conv, [64, 3, 1, 1]],  # 1-Conv1_2
   [-1, 1, MaxPool, [2, 2, 0]],   # 2-MaxPool1

   # Block 2
   [-1, 1, Conv, [128, 3, 1, 1]], # 3-Conv2_1
   [-1, 1, Conv, [128, 3, 1, 1]], # 4-Conv2_2
   [-1, 1, MaxPool, [2, 2, 0]],   # 5-MaxPool2

   # Block 3
   [-1, 1, Conv, [256, 3, 1, 1]], # 6-Conv3_1
   [-1, 1, Conv, [256, 3, 1, 1]], # 7-Conv3_2
   [-1, 1, Conv, [256, 3, 1, 1]], # 8-Conv3_3
   [-1, 1, MaxPool, [2, 2, 0]],   # 9-MaxPool3

   # Block 4
   [-1, 1, Conv, [512, 3, 1, 1]], # 10-Conv4_1
   [-1, 1, Conv, [512, 3, 1, 1]], # 11-Conv4_2
   [-1, 1, Conv, [512, 3, 1, 1]], # 12-Conv4_3
   [-1, 1, MaxPool, [2, 2, 0]],   # 13-MaxPool4

   # Block 5
   [-1, 1, Conv, [512, 3, 1, 1]], # 14-Conv5_1
   [-1, 1, Conv, [512, 3, 1, 1]], # 15-Conv5_2
   [-1, 1, Conv, [512, 3, 1, 1]], # 16-Conv5_3
   [-1, 1, MaxPool, [2, 2, 0]],   # 17-MaxPool5
  ]

# head
head:
  [
   [-1, 1, Linear, [4096]],      # 18-FC1
   [-1, 1, ReLU, []],            # ReLU activation
   [-1, 1, Dropout, [0.5]],      # Dropout

   [-1, 1, Linear, [4096]],      # 19-FC2
   [-1, 1, ReLU, []],            # ReLU activation
   [-1, 1, Dropout, [0.5]],      # Dropout

   [-1, 1, Linear, [nc]],        # 20-FC3
   [-1, 1, Softmax, []],         # Softmax activation for classification
  ]
